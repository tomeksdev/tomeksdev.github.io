---
title: Huawei Dorado 3000 V6 Benchmark on Proxmox VE  
description: A deep‚Äëdive benchmark of Huawei OceanStor Dorado 3000 V6 on Proxmox VE using iSCSI, NFS, and Multipath configurations  
author: vujca  
date: 2025-11-16 10:00:00 +0100  
categories: [Testing]  
tags: [proxmox, storage, benchmark, dorado, iscsi, nfs, linux, virtualization]  
image:  
  path: /assets/img/post/Huawei-dorado/huawei-dorado.png  
  alt: Huawei Dorado 3000 V6  
---

Storage performance often makes the difference between a sluggish virtual environment and a high‚Äëperforming one. In this benchmark, I tested the **Huawei OceanStor Dorado 3000 V6** ‚Äî an all‚Äëflash enterprise storage array ‚Äî within a **Proxmox VE 9.0.4** virtualized environment. The goal? To compare NFS, iSCSI, and iSCSI Multipath (MPIO) configurations.

> The results? iSCSI MPIO *crushed it* ‚Äî delivering over **500‚ÄØ000 IOPS** with ultra‚Äëlow latency.

---

## üõ†Ô∏è Test Lab Configuration

To replicate a realistic data‚Äëcenter setup, the following hardware and software were used:

| Component     | Specification |
|--------------|--------------|
| **Servers**   | 2 √ó HPE ProLiant DL380 |
| **CPU**       | 2 √ó Intel¬Æ Xeon¬Æ Gold 5418Y |
| **Memory**    | 512‚ÄØGB RAM |
| **System Disks** | 2 √ó 960‚ÄØGB SSD in RAID 1 |
| **Storage**   | Huawei OceanStor Dorado 3000 V6 (4 √ó 10‚ÄØGbE per controller) |
| **Networking** | Redundant 25‚ÄØGbE iSCSI and NFS VLANs |
| **Hypervisor** | Proxmox VE 9.0.4 |
| **VMs**       | Debian‚Äëbased, using `fio 3.39` |
| **Drivers**   | VirtIO‚ÄëSCSI & VirtIO‚ÄëNet |
| **Benchmark Tool** | `fio` (Flexible I/O Tester) |

---

## üìà Benchmarking Methodology

To simulate both large data transfers and typical virtualized workloads, two `fio` profiles were used:

### Sequential Read/Write (1‚ÄØM block)
```bash
fio --rw=rw --bs=1M --iodepth=16 --size=10G --runtime=60 ...
```

### Random Read/Write (4‚ÄØK, 70/30 mix)
```bash
fio --rw=randrw --rwmixread=70 --bs=4k --iodepth=32 ...
```

---

## üìä Performance Summary

| Protocol             | Sequential R/W | Random R/W | IOPS    | Avg Latency |
|----------------------|----------------|------------|---------|-------------|
| **NFS**              | 2.3‚ÄØGB/s       | 30‚ÄØMB/s    | ~7.7‚ÄØK  | 5‚Äì10‚ÄØms     |
| **iSCSI (Data Protect)** | 8.8‚ÄØGB/s       | 530‚ÄØMB/s   | ~120‚ÄØK | 0.6‚ÄØms      |
| **iSCSI (Normal)**   | 8.0‚ÄØGB/s       | 520‚ÄØMB/s   | ~120‚ÄØK  | 0.6‚ÄØms      |
| **iSCSI MPIO (mpath0)** | 3.0‚ÄØGB/s       | 7.2‚ÄØGB/s   | ~580‚ÄØK | 0.07‚ÄØms     |
| **iSCSI MPIO (mpath1)** | 3.4‚ÄØGB/s       | 6.7‚ÄØGB/s   | ~350‚ÄØK | 0.08‚ÄØms     |

üìå **Highlight:** MPIO achieved over **500‚ÄØ000 IOPS** with **sub‚Äë0.1‚ÄØms latency** ‚Äî exceptional for virtual machines and database workloads.

---

## üìâ Visual Performance Chart

![Huawei Dorado Benchmark Results](/assets/img/post/Huawei-dorado/output.png)

> Side‚Äëby‚Äëside throughput & IOPS comparison of NFS vs. iSCSI vs. MPIO

---

## üîç Observations & Insights

### NFS
- Offers decent sequential performance, but **random I/O becomes a bottleneck**.  
- Caused by single‚Äëthreaded metadata operations.  
- ‚úÖ Best suited for archival or light‚Äëuse workloads.

### iSCSI (Single Path)
- Nearly fully utilizes array performance.  
- Low latency (<‚ÄØ1‚ÄØms) ‚Äî ideal for production VMs.  
- Minimal overhead even with **Data Protection** enabled.

### iSCSI MPIO
- Utilizes both 25‚ÄØGbE links simultaneously.  
- Delivers **enterprise‚Äëclass random I/O performance**.  
- Best choice for latency‚Äësensitive VMs and clustered databases.

---

## üß∞ Recommended Tuning

### Multipath Configuration (`/etc/multipath.conf`)
```conf
defaults {
  user_friendly_names yes
  polling_interval 5
}
devices {
  device {
    vendor "HUAWEI"
    product "XSG1"
    path_grouping_policy multibus
    path_checker tur
    no_path_retry queue
    rr_min_io_rq 1
    rr_weight uniform
  }
}
```

### NFS Mount Example
```bash
mount -t nfs -o vers=4.1,nconnect=8,noatime,rsize=1048576,wsize=1048576 <server>:/volume /mnt/test
```

### Performance Optimization
```bash
echo none | tee /sys/block/sdX/queue/scheduler
echo 3 | tee /proc/sys/vm/drop_caches
```

---

## ‚úÖ Conclusion

The **Huawei Dorado 3000 V6** proves to be a **powerful and efficient storage backend** for Proxmox VE environments. While NFS is easy to set up, it‚Äôs not suitable for workloads with demanding random I/O. On the other hand, iSCSI ‚Äî especially when paired with MPIO ‚Äî unleashes the full potential of this all‚Äëflash array.

### Summary
- **Top performer:** iSCSI MPIO ‚Äî over 500‚ÄØ000 IOPS with latency below 0.1‚ÄØms  
- **Good choice:** iSCSI single path for smaller or less demanding setups  
- **Use with caution:** NFS for workloads involving heavy random I/O

If you‚Äôre building a **high‚Äëperformance virtualized infrastructure**, Huawei Dorado 3000 V6 with properly tuned iSCSI multipath is a **clear winner**.

---

## üîÅ (Optional) fio Benchmark Script

```bash
#!/bin/bash
DISK="/dev/mpath0"
RUNTIME=60

fio --name=seq_rw --rw=rw --bs=1M --numjobs=1 --iodepth=16 --size=10G --runtime=$RUNTIME --filename=$DISK --direct=1 --ioengine=libaio --group_reporting --output=seq_rw.log

fio --name=randrw --rw=randrw --rwmixread=70 --bs=4k --numjobs=4 --iodepth=32 --size=10G --runtime=$RUNTIME --filename=$DISK --direct=1 --ioengine=libaio --group_reporting --output=randrw.log
```

---

Want to know how to set this up in your environment? Drop a comment or get in touch!
